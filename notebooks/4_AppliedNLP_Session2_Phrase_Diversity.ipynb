{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6e32df",
   "metadata": {},
   "source": [
    "# 4) Phrase Diversity (n-gram TTR)\n",
    "\n",
    "This notebook is part of **Applied NLP – Session 2: Phrases & Collocations**.\n",
    "\n",
    "Overview:\n",
    "- Measure phrase diversity using Type-Token Ratio (TTR) for bigrams and trigrams.\n",
    "- Compare lexical variety across two works by the same author.\n",
    "- (Optional) Analyze diversity trends across sections/chapters within the texts.\n",
    "\n",
    "Learning objectives:\n",
    "- Understand and compute Type-Token Ratio (TTR) as a measure of lexical/phrasal diversity.\n",
    "- Apply TTR to n-grams (bigrams, trigrams) to quantify phrase variety.\n",
    "- Interpret TTR values: lower TTR suggests formulaic/repetitive phrasing, higher TTR indicates more varied expression.\n",
    "- Produce reproducible diversity metrics and visualizations for literary analysis.\n",
    "\n",
    "Quick start:\n",
    "1. Edit the `CONFIG` dictionary in the next code cell to point to your two plain-text books.\n",
    "2. (Optional) Toggle `use_stopwords` to exclude common function words.\n",
    "3. Run cells from top to bottom. The main outputs are saved to `../results/`.\n",
    "4. (Optional) Customize section-splitting regex to analyze diversity by chapter.\n",
    "\n",
    "Prerequisites:\n",
    "- A Python environment with requirements.txt packages installed (pandas, matplotlib).\n",
    "- The text files for the two works placed in `../data/`.\n",
    "\n",
    "Notes and tips:\n",
    "- The notebook uses the same robust preprocessing as notebooks 1-2 (strip_gutenberg, normalize quotes, etc.).\n",
    "- TTR formula: (unique n-grams) / (total n-grams). Values range from 0 to 1.\n",
    "- Lower TTR: more repetition (e.g., epic poetry with formulaic epithets like \"wine-dark sea\").\n",
    "- Higher TTR: more varied phrasing (e.g., experimental modernist prose).\n",
    "- Section-wise analysis (optional cell) helps identify where diversity changes within a work.\n",
    "- Compare TTR between your two books to see if the author's style evolved or differs by genre.\n",
    "- Stopword removal may artificially inflate TTR by reducing common bigrams like \"of the\" or \"in the\".\n",
    "\n",
    "**Goal:** Apply Type-Token Ratio (TTR) to bigrams and trigrams to measure phrase diversity in your two selected works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff57e43",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration\n",
    "\n",
    "- Fill the `CONFIG` paths for your two books (plain text).\n",
    "- Toggle stopwords and thresholds as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85f2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Imports & Config =====\n",
    "import re, os, math, json, collections\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "CONFIG = {\n",
    "    \"book1_path\": \"../data/Wonderland.txt\",  # <-- change\n",
    "    \"book2_path\": \"../data/Looking-Glass.txt\",  # <-- change\n",
    "    \"language\": \"en\",                # e.g. 'en','de','ru','el'\n",
    "    \"use_stopwords\": False,          # toggle\n",
    "    \"min_ngram_count\": 5,            # threshold (where applicable)\n",
    "    \"top_k\": 20                      # top items to show\n",
    "}\n",
    "\n",
    "# Unicode-aware token regex: words with optional internal ' or -\n",
    "WORD_RE = re.compile(r\"[^\\W\\d_]+(?:[-'][^\\W\\d_]+)*\", flags=re.UNICODE)\n",
    "\n",
    "# Optional: supply your own stopwords set per language\n",
    "STOPWORDS = set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456952dc",
   "metadata": {},
   "source": [
    "## 1. Load & Normalize Text\n",
    "\n",
    "- Fix hyphenated line breaks (e.g., end-of-line hyphens).\n",
    "- Normalize whitespace.\n",
    "- Lowercase consistently.\n",
    "\n",
    "Our books are a part of Project Gutenberg, which means there are some extra texts in each txt file to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a861df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust Project Gutenberg boilerplate stripper --------------------------\n",
    "_GB_START_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",   # modern\n",
    "    r\"START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",             # fallback\n",
    "    r\"End of the Project Gutenberg(?:'s)? Etext\",               # very old variants sometimes inverted\n",
    "]\n",
    "_GB_END_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",      # modern\n",
    "    r\"END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",                # fallback\n",
    "    r\"End of Project Gutenberg(?:'s)? (?:Etext|eBook)\",          # older variants\n",
    "    r\"\\*\\*\\*\\s*END: FULL LICENSE\\s*\\*\\*\\*\",                      # license block end (older)\n",
    "]\n",
    "\n",
    "# Chapters (heuristic fallback if markers missing; English-centric but works often)\n",
    "_CHAPTER_HINTS = [\n",
    "    r\"^\\s*chapter\\s+[ivxlcdm0-9]+[\\.\\: ]\",   # CHAPTER I / Chapter 1\n",
    "    r\"^\\s*book\\s+[ivxlcdm0-9]+[\\.\\: ]\",      # BOOK I etc.\n",
    "    r\"^\\s*part\\s+[ivxlcdm0-9]+[\\.\\: ]\",\n",
    "]\n",
    "\n",
    "def strip_gutenberg(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns text between Gutenberg START and END markers (case-insensitive).\n",
    "    If markers aren't found, heuristically trims to first chapter-like heading.\n",
    "    Works for most EN/DE/RU/EL releases since headers are in English.\n",
    "    \"\"\"\n",
    "    t = text.replace(\"\\ufeff\", \"\")  # strip BOM if present\n",
    "\n",
    "    # Find START\n",
    "    start_idx = None\n",
    "    for pat in _GB_START_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # start AFTER the matched line\n",
    "            start_idx = t.find(\"\\n\", m.end())\n",
    "            if start_idx == -1:\n",
    "                start_idx = m.end()\n",
    "            break\n",
    "\n",
    "    # Find END\n",
    "    end_idx = None\n",
    "    for pat in _GB_END_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # end BEFORE the matched line\n",
    "            end_idx = m.start()\n",
    "            break\n",
    "\n",
    "    if start_idx is not None and end_idx is not None and end_idx > start_idx:\n",
    "        core = t[start_idx:end_idx]\n",
    "    else:\n",
    "        # Fallback: try to start at first chapter-like heading\n",
    "        core = t\n",
    "        for pat in _CHAPTER_HINTS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE | re.MULTILINE)\n",
    "            if m:\n",
    "                core = core[m.start():]\n",
    "                break\n",
    "        # And trim off the standard license tail if present\n",
    "        for pat in _GB_END_MARKERS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE)\n",
    "            if m:\n",
    "                core = core[:m.start()]\n",
    "                break\n",
    "\n",
    "    # Remove license/contact blocks that sometimes sneak inside\n",
    "    core = re.sub(r\"\\n\\s*End of the Project Gutenberg.*\", \"\", core, flags=re.IGNORECASE)\n",
    "    core = re.sub(r\"\\*\\*\\*\\s*START: FULL LICENSE\\s*\\*\\*\\*.*\", \"\", core, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # Clean leftover cruft: URLs, repeated separators\n",
    "    core = re.sub(r\"https?://\\S+\", \"\", core)\n",
    "    core = re.sub(r\"[ \\t]+\\n\", \"\\n\", core)   # trailing spaces before newline\n",
    "    core = re.sub(r\"\\n{3,}\", \"\\n\\n\", core)   # collapse big blank blocks\n",
    "    return core.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d30a5630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58137,\n",
       " ['illustration',\n",
       "  'alice',\n",
       "  's',\n",
       "  'adventures',\n",
       "  'in',\n",
       "  'wonderland',\n",
       "  'by',\n",
       "  'lewis',\n",
       "  'carroll',\n",
       "  'the',\n",
       "  'millennium',\n",
       "  'fulcrum'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_text(p: str) -> str:\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    # 1) strip Gutenberg header/footer FIRST\n",
    "    t = strip_gutenberg(t)\n",
    "    # 2) join hyphenated line breaks (e.g., \"won-\\nderful\")\n",
    "    t = re.sub(r\"-\\s*\\n\", \"\", t)\n",
    "    # 3) normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))\n",
    "text2 = normalize_text(load_text(CONFIG[\"book2_path\"]))\n",
    "\n",
    "tokens1 = WORD_RE.findall(text1.lower())\n",
    "tokens2 = WORD_RE.findall(text2.lower())\n",
    "\n",
    "if CONFIG[\"use_stopwords\"]:\n",
    "    tokens1 = [t for t in tokens1 if t not in STOPWORDS]\n",
    "    tokens2 = [t for t in tokens2 if t not in STOPWORDS]\n",
    "\n",
    "tokens = tokens1 + tokens2\n",
    "\n",
    "len(tokens1), len(tokens2), len(tokens)\n",
    "\n",
    "len(tokens), tokens[:12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e3bc71",
   "metadata": {},
   "source": [
    "## 2. Compute TTR for Bigrams & Trigrams\n",
    "\n",
    "Type–Token Ratio for n-grams measures phrase variety.\n",
    "\n",
    "\\[ \\text{TTR}(S) = \\frac{|\\text{unique n-grams}|}{|\\text{total n-grams}|} \\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da4d4191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bigram_TTR</td>\n",
       "      <td>0.461521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trigram_TTR</td>\n",
       "      <td>0.817528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        metric     value\n",
       "0   bigram_TTR  0.461521\n",
       "1  trigram_TTR  0.817528"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ttr(seq):\n",
    "    return len(set(seq)) / max(1, len(seq))\n",
    "\n",
    "bigrams_list = list(zip(tokens, tokens[1:]))\n",
    "trigrams_list = list(zip(tokens, tokens[1:], tokens[2:]))\n",
    "\n",
    "ttr2 = ttr(bigrams_list)\n",
    "ttr3 = ttr(trigrams_list)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"metric\": [\"bigram_TTR\", \"trigram_TTR\"],\n",
    "    \"value\": [ttr2, ttr3]\n",
    "})\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f955f",
   "metadata": {},
   "source": [
    "## 3. (Optional) Section-wise Diversity\n",
    "\n",
    "If chapters/sections are detectable by regex, estimate diversity per section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6289e32",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# naive split by keywords; customize per book\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sections \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbchapter\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbbook\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbpart\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mfull_text\u001b[49m, flags\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39mIGNORECASE)\n\u001b[1;32m      3\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sections, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_text' is not defined"
     ]
    }
   ],
   "source": [
    "# naive split by keywords; customize per book\n",
    "sections = re.split(r\"\\bchapter\\b|\\bbook\\b|\\bpart\\b\", full_text, flags=re.IGNORECASE)\n",
    "rows = []\n",
    "for i, sec in enumerate(sections, start=1):\n",
    "    toks = WORD_RE.findall(sec.lower())\n",
    "    b2 = list(zip(toks, toks[1:]))\n",
    "    b3 = list(zip(toks, toks[1:], toks[2:]))\n",
    "    rows.append({\"section\": i, \"bigram_TTR\": ttr(b2), \"trigram_TTR\": ttr(b3)})\n",
    "sec_df = pd.DataFrame(rows)\n",
    "\n",
    "ax = sec_df.plot(x=\"section\", y=[\"bigram_TTR\",\"trigram_TTR\"])\n",
    "ax.set_title(\"Phrase Diversity by Section (proxy)\")\n",
    "ax.set_xlabel(\"Section index\")\n",
    "ax.set_ylabel(\"TTR\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0853b1a0",
   "metadata": {},
   "source": [
    "## 4. Notes\n",
    "\n",
    "- Lower TTR suggests more formulaic phrasing (e.g., epic epithets).\n",
    "- Compare the two works separately if you can split cleanly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ebc85b",
   "metadata": {},
   "source": [
    "## 5. Reflection (Answer in your repo's README or below)\n",
    "\n",
    "- Which results matched your reading intuition?\n",
    "- What surprised you?\n",
    "- If you toggled preprocessing (stopwords on/off), what changed?\n",
    "- Compare across the two works: are the patterns stable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea5f239",
   "metadata": {},
   "source": [
    "## 6. Export (tables/figures)\n",
    "\n",
    "This cell saves outputs into the `../results/` folder so you can add them to your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7ef385d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 900x450 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Path(\"../results\").mkdir(exist_ok=True)\n",
    "summary_df.to_csv(f\"../results/TTR_table.csv\", index=False)\n",
    "plt.savefig(f\"../results/TTR_figure.png\", dpi=200, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30dc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
