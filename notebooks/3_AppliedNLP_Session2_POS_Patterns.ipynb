{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc65d68b",
   "metadata": {},
   "source": [
    "# 3) POS Pattern Frequency (Adj+N, V+N, etc.)\n",
    "\n",
    "This notebook is part of **Applied NLP – Session 2: Phrases & Collocations**.\n",
    "\n",
    "Overview:\n",
    "- Analyze grammatical patterns in bigrams using Part-of-Speech (POS) tagging.\n",
    "- Identify common POS patterns like ADJ+NOUN, VERB+NOUN, NOUN+NOUN across two works by the same author.\n",
    "- Visualize the distribution of POS patterns to understand syntactic phrase structure.\n",
    "\n",
    "Learning objectives:\n",
    "- Apply spaCy POS tagging to tokenized text for grammatical analysis.\n",
    "- Compute and compare POS bigram patterns across literary texts.\n",
    "- Visualize syntactic patterns to identify stylistic features.\n",
    "- Understand how preprocessing choices (stopwords, filtering) affect syntactic analysis.\n",
    "\n",
    "Quick start:\n",
    "1. Edit the `CONFIG` dictionary in the next code cell to point to your two plain-text books.\n",
    "2. (Optional) Toggle `use_stopwords` to remove common function words.\n",
    "3. Run cells from top to bottom. The main outputs are saved to `../results/`.\n",
    "4. Ensure `en_core_web_sm` spaCy model is installed (included in requirements.txt).\n",
    "\n",
    "Prerequisites:\n",
    "- A Python environment with requirements.txt packages installed (pandas, matplotlib, spacy).\n",
    "- spaCy English model: `en_core_web_sm` (should be installed via requirements.txt).\n",
    "- The text files for the two works placed in `../data/`.\n",
    "\n",
    "Notes and tips:\n",
    "- The notebook uses the same robust preprocessing as notebooks 1-2 (strip_gutenberg, normalize quotes, etc.).\n",
    "- POS tagging can be slow on large texts; consider slicing tokens or processing in chunks.\n",
    "- Common patterns: ADJ+NOUN (descriptive phrases), VERB+NOUN (action phrases), NOUN+NOUN (compounds).\n",
    "- Compare patterns between your two books to see if syntactic style differs.\n",
    "- For non-English texts, change the spaCy model in the CONFIG or POS tagging cell (e.g., `de_core_news_sm` for German).\n",
    "\n",
    "**Goal:** Identify and visualize the most frequent Part-of-Speech bigram patterns (e.g., ADJ+NOUN, VERB+NOUN) in your two selected works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d413dc",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration\n",
    "\n",
    "- Fill the `CONFIG` paths for your two books (plain text).\n",
    "- Toggle stopwords and thresholds as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0cf353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Imports & Config =====\n",
    "import re, os, math, json, collections\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "CONFIG = {\n",
    "    \"book1_path\": \"../data/Wonderland.txt\",  # <-- change\n",
    "    \"book2_path\": \"../data/Looking-Glass.txt\",  # <-- change\n",
    "    \"language\": \"en\",                # e.g. 'en','de','ru','el'\n",
    "    \"use_stopwords\": False,          # toggle\n",
    "    \"min_ngram_count\": 5,            # threshold (where applicable)\n",
    "    \"top_k\": 20                      # top items to show\n",
    "}\n",
    "\n",
    "# Unicode-aware token regex: words with optional internal ' or -\n",
    "WORD_RE = re.compile(r\"[^\\W\\d_]+(?:[-'][^\\W\\d_]+)*\", flags=re.UNICODE)\n",
    "\n",
    "# Optional: supply your own stopwords set per language\n",
    "STOPWORDS = set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd3fb5",
   "metadata": {},
   "source": [
    "## 1. Load & Normalize Text\n",
    "\n",
    "- Fix hyphenated line breaks (e.g., end-of-line hyphens).\n",
    "- Normalize whitespace.\n",
    "- Lowercase consistently.\n",
    "\n",
    "Our books are a part of Project Gutenberg, which means there are some extra texts in each txt file to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e36da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust Project Gutenberg boilerplate stripper --------------------------\n",
    "_GB_START_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",   # modern\n",
    "    r\"START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",             # fallback\n",
    "    r\"End of the Project Gutenberg(?:'s)? Etext\",               # very old variants sometimes inverted\n",
    "]\n",
    "_GB_END_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",      # modern\n",
    "    r\"END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",                # fallback\n",
    "    r\"End of Project Gutenberg(?:'s)? (?:Etext|eBook)\",          # older variants\n",
    "    r\"\\*\\*\\*\\s*END: FULL LICENSE\\s*\\*\\*\\*\",                      # license block end (older)\n",
    "]\n",
    "\n",
    "# Chapters (heuristic fallback if markers missing; English-centric but works often)\n",
    "_CHAPTER_HINTS = [\n",
    "    r\"^\\s*chapter\\s+[ivxlcdm0-9]+[\\.\\: ]\",   # CHAPTER I / Chapter 1\n",
    "    r\"^\\s*book\\s+[ivxlcdm0-9]+[\\.\\: ]\",      # BOOK I etc.\n",
    "    r\"^\\s*part\\s+[ivxlcdm0-9]+[\\.\\: ]\",\n",
    "]\n",
    "\n",
    "def strip_gutenberg(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns text between Gutenberg START and END markers (case-insensitive).\n",
    "    If markers aren't found, heuristically trims to first chapter-like heading.\n",
    "    Works for most EN/DE/RU/EL releases since headers are in English.\n",
    "    \"\"\"\n",
    "    t = text.replace(\"\\ufeff\", \"\")  # strip BOM if present\n",
    "\n",
    "    # Find START\n",
    "    start_idx = None\n",
    "    for pat in _GB_START_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # start AFTER the matched line\n",
    "            start_idx = t.find(\"\\n\", m.end())\n",
    "            if start_idx == -1:\n",
    "                start_idx = m.end()\n",
    "            break\n",
    "\n",
    "    # Find END\n",
    "    end_idx = None\n",
    "    for pat in _GB_END_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # end BEFORE the matched line\n",
    "            end_idx = m.start()\n",
    "            break\n",
    "\n",
    "    if start_idx is not None and end_idx is not None and end_idx > start_idx:\n",
    "        core = t[start_idx:end_idx]\n",
    "    else:\n",
    "        # Fallback: try to start at first chapter-like heading\n",
    "        core = t\n",
    "        for pat in _CHAPTER_HINTS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE | re.MULTILINE)\n",
    "            if m:\n",
    "                core = core[m.start():]\n",
    "                break\n",
    "        # And trim off the standard license tail if present\n",
    "        for pat in _GB_END_MARKERS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE)\n",
    "            if m:\n",
    "                core = core[:m.start()]\n",
    "                break\n",
    "\n",
    "    # Remove license/contact blocks that sometimes sneak inside\n",
    "    core = re.sub(r\"\\n\\s*End of the Project Gutenberg.*\", \"\", core, flags=re.IGNORECASE)\n",
    "    core = re.sub(r\"\\*\\*\\*\\s*START: FULL LICENSE\\s*\\*\\*\\*.*\", \"\", core, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # Clean leftover cruft: URLs, repeated separators\n",
    "    core = re.sub(r\"https?://\\S+\", \"\", core)\n",
    "    core = re.sub(r\"[ \\t]+\\n\", \"\\n\", core)   # trailing spaces before newline\n",
    "    core = re.sub(r\"\\n{3,}\", \"\\n\\n\", core)   # collapse big blank blocks\n",
    "    return core.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(p: str) -> str:\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    # 1) strip Gutenberg header/footer FIRST\n",
    "    t = strip_gutenberg(t)\n",
    "    # 2) join hyphenated line breaks (e.g., \"won-\\nderful\")\n",
    "    t = re.sub(r\"-\\s*\\n\", \"\", t)\n",
    "    # 3) normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))\n",
    "text2 = normalize_text(load_text(CONFIG[\"book2_path\"]))\n",
    "\n",
    "tokens1 = WORD_RE.findall(text1.lower())\n",
    "tokens2 = WORD_RE.findall(text2.lower())\n",
    "\n",
    "if CONFIG[\"use_stopwords\"]:\n",
    "    tokens1 = [t for t in tokens1 if t not in STOPWORDS]\n",
    "    tokens2 = [t for t in tokens2 if t not in STOPWORDS]\n",
    "\n",
    "tokens = tokens1 + tokens2\n",
    "\n",
    "len(tokens1), len(tokens2), len(tokens)\n",
    "\n",
    "len(tokens), tokens[:12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925688df",
   "metadata": {},
   "source": [
    "## 2. POS Tagging Setup\n",
    "\n",
    "Uses spaCy. For English: `en_core_web_sm`. For other languages, switch to an appropriate model. `en_core_web_sm` should be downloaded for you if you followed the requirements.txt requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Change model as needed (e.g., 'de_core_news_sm', 'ru_core_news_sm')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_pos_patterns(token_list, nlp_model):\n",
    "    \"\"\"Extract POS bigram patterns from a list of tokens.\"\"\"\n",
    "    doc = nlp_model(\" \".join(token_list))\n",
    "    pairs = []\n",
    "    for i in range(len(doc)-1):\n",
    "        a, b = doc[i], doc[i+1]\n",
    "        if a.is_alpha and b.is_alpha:\n",
    "            pairs.append(f\"{a.pos_}+{b.pos_}\")\n",
    "    return Counter(pairs)\n",
    "\n",
    "# Combined corpus POS patterns\n",
    "print(\"Analyzing combined corpus...\")\n",
    "pat_counts_combined = extract_pos_patterns(tokens, nlp)\n",
    "pos_df_combined = (pd.DataFrame(pat_counts_combined.items(), columns=[\"pattern\",\"count\"])\n",
    "                   .sort_values(\"count\", ascending=False)\n",
    "                   .head(CONFIG[\"top_k\"])\n",
    "                   .reset_index(drop=True))\n",
    "\n",
    "# Book 1 POS patterns\n",
    "print(\"Analyzing Book 1...\")\n",
    "pat_counts_1 = extract_pos_patterns(tokens1, nlp)\n",
    "pos_df_1 = (pd.DataFrame(pat_counts_1.items(), columns=[\"pattern\",\"count\"])\n",
    "            .sort_values(\"count\", ascending=False)\n",
    "            .head(CONFIG[\"top_k\"])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "# Book 2 POS patterns\n",
    "print(\"Analyzing Book 2...\")\n",
    "pat_counts_2 = extract_pos_patterns(tokens2, nlp)\n",
    "pos_df_2 = (pd.DataFrame(pat_counts_2.items(), columns=[\"pattern\",\"count\"])\n",
    "            .sort_values(\"count\", ascending=False)\n",
    "            .head(CONFIG[\"top_k\"])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "print(\"POS analysis complete\\n\")\n",
    "\n",
    "# Display combined patterns\n",
    "pos_df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969f5358",
   "metadata": {},
   "source": [
    "## 3. Visualize Top POS Patterns\n",
    "\n",
    "### 3.1 Combined Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3654e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pos_df_combined.plot.barh(x=\"pattern\", y=\"count\", legend=False, color='steelblue')\n",
    "ax.invert_yaxis()\n",
    "ax.set_title(\"Top POS Bigram Patterns - Combined Corpus\")\n",
    "ax.set_xlabel(\"Count\")\n",
    "# Capture the figure so we can save it later\n",
    "fig_pos_combined = ax.get_figure()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d096c",
   "metadata": {},
   "source": [
    "### 3.2 Per-Book Comparison\n",
    "\n",
    "Compare POS patterns across the two works to see if syntactic style differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd347fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pos_compare, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Book 1\n",
    "ax1 = axes[0]\n",
    "pos_df_1.plot.barh(x=\"pattern\", y=\"count\", legend=False, color='steelblue', ax=ax1)\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_title(\"Book 1: Top POS Patterns\", fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel(\"Count\")\n",
    "ax1.set_ylabel(\"POS Pattern\")\n",
    "\n",
    "# Book 2\n",
    "ax2 = axes[1]\n",
    "pos_df_2.plot.barh(x=\"pattern\", y=\"count\", legend=False, color='coral', ax=ax2)\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_title(\"Book 2: Top POS Patterns\", fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel(\"Count\")\n",
    "ax2.set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19673129",
   "metadata": {},
   "source": [
    "## 4. Notes\n",
    "\n",
    "- Useful patterns: `ADJ+NOUN`, `NOUN+NOUN`, `VERB+NOUN`, `PRON+VERB`.\n",
    "- The notebook now analyzes POS patterns for both books individually and for the combined corpus.\n",
    "- Compare the per-book visualizations to identify syntactic style differences between the two works.\n",
    "\n",
    "### Interpretation Tips\n",
    "\n",
    "Comparing POS patterns across works by the same author can reveal:\n",
    "\n",
    "1. **Stylistic consistency**: If both books show similar POS pattern distributions, the author has a consistent syntactic style across works.\n",
    "\n",
    "2. **Genre or thematic differences**: Variations in patterns like `ADJ+NOUN` vs. `NOUN+NOUN` may reflect different narrative approaches—more descriptive vs. more action-oriented writing.\n",
    "\n",
    "3. **Character-driven vs. narration-driven prose**: Higher `PRON+VERB` counts suggest dialogue-heavy or character-focused sections, while `ADJ+NOUN` dominance indicates descriptive narration.\n",
    "\n",
    "4. **Syntactic complexity**: The diversity and distribution of patterns can indicate sentence structure complexity—varied patterns suggest more sophisticated syntax.\n",
    "\n",
    "5. **Temporal evolution**: If the books were written at different times, pattern differences may reflect the author's stylistic development or changing literary influences.\n",
    "\n",
    "For example, if Book 1 shows more `ADJ+NOUN` patterns while Book 2 has more `VERB+NOUN`, this might suggest a shift from descriptive to action-oriented writing style.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a28c2d",
   "metadata": {},
   "source": [
    "## 5. Reflection (Answer in your repo's README or below)\n",
    "\n",
    "- Which results matched your reading intuition?\n",
    "- What surprised you?\n",
    "- If you toggled preprocessing (stopwords on/off), what changed?\n",
    "- Compare across the two works: are the patterns stable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2135cd",
   "metadata": {},
   "source": [
    "## 6. Export (tables/figures)\n",
    "\n",
    "This cell saves outputs into the `../results/` folder so you can add them to your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054845ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"../results\").mkdir(exist_ok=True)\n",
    "\n",
    "# Save CSV tables\n",
    "pos_df_combined.to_csv(\"../results/POS_patterns_combined.csv\", index=False)\n",
    "pos_df_1.to_csv(\"../results/POS_patterns_book1.csv\", index=False)\n",
    "pos_df_2.to_csv(\"../results/POS_patterns_book2.csv\", index=False)\n",
    "\n",
    "# Save figures\n",
    "saved_files = []\n",
    "try:\n",
    "    fig_pos_combined.savefig(\"../results/POS_patterns_combined.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    saved_files.append(\"POS_patterns_combined.png\")\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    fig_pos_compare.savefig(\"../results/POS_patterns_comparison.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    saved_files.append(\"POS_patterns_comparison.png\")\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "if saved_files:\n",
    "    print(f\"✓ Saved CSV tables and figures to ../results/:\")\n",
    "    print(f\"  - POS_patterns_combined.csv, POS_patterns_book1.csv, POS_patterns_book2.csv\")\n",
    "    print(f\"  - {', '.join(saved_files)}\")\n",
    "else:\n",
    "    print(\"⚠ Figures not saved - run the visualization cells first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2905a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
